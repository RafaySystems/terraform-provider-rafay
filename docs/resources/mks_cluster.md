---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "rafay_mks_cluster Resource - rafay"
subcategory: ""
description: |-
  
---

# rafay_mks_cluster (Resource)

## Example usage

### Basic Single node Cluster
```terraform
resource "rafay_mks_cluster" "mks-noha-converged-cluster" {
  api_version = "infra.k8smgmt.io/v3"
  kind        = "Cluster"
  metadata = {
    name    = "mks-noha-converged-cluster"
    project = "terraform"
  }
  spec = {
    blueprint = {
      name = "minimal"
    }
    config = {
      auto_approve_nodes      = true
      kubernetes_version      = "v1.28.9"
      installer_ttl           = 365
      platform_version        = "v1.1.0"
      network = {
        cni = {
          name    = "Calico"
          version = "3.26.1"
        }
        pod_subnet     = "10.244.0.0/16"
        service_subnet = "10.96.0.0/12"
      }
      cluster_ssh = {
        username         = "ubuntu"
        port             = "22"
        private_key_path = "/path/to/privatekey/"
      }
      nodes = {
        "hostname1" = {
          arch             = "amd64"
          hostname         = "hostname1"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.234"
          roles            = ["ControlPlane", "Worker"]
          ssh = {
            ip_address = "129.146.203.73"
          }
        }
      }
    }
  }
}
```


### Example HA cluster with Converged Control Plane nodes

In this example, control plane nodes will also be used for workload placement, and since `spec.cloud_credentials` are provided, you can perform Terraform operations from anywhere outside the node's network.

```terraform
resource "rafay_mks_cluster" "mks-ha-cluster" {
  api_version = "infra.k8smgmt.io/v3"
  kind        = "Cluster"
  metadata = {
    name    = "mks-ha-cluster"
    project = "terraform"
  }
  spec = {
    blueprint = {
      name = "minimal"
    }
    cloud_credentials = "mks-ssh-credentials"
    config = {
      auto_approve_nodes      = true
      high_availability       = true
      kubernetes_version      = "v1.28.9"
      installer_ttl           = 365
      platform_version        = "v1.1.0"
      kubernetes_upgrade = {
        strategy = "sequential"
        params = {
          worker_concurrency = "50%"
        }
      }
      network = {
        cni = {
          name    = "Calico"
          version = "3.26.1"
        }
        pod_subnet     = "10.244.0.0/16"
        service_subnet = "10.96.0.0/12"
      }
      nodes = {
        "hostname1" = {
          arch             = "amd64"
          hostname         = "hostname1"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.233"
          roles            = ["ControlPlane", "Worker"]
          labels = {
            "app"   = "infra"
            "infra" = "true"
          }
        },
        "hostname2" = {
          arch             = "amd64"
          hostname         = "hostname2"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.234"
          roles            = ["ControlPlane", "Worker"]
        },
        "hostname3" = {
          arch             = "amd64"
          hostname         = "hostname3"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.235"
          roles            = ["ControlPlane", "Worker"]
        },
        "hostname4" = {
          arch             = "amd64"
          hostname         = "hostname4"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.114.236"
          roles            = ["Worker"]
        }
      }
    }
  }
}
```


### Example HA Cluster Having Dedicated Control Plane with System Component Placement

In this example, the cluster is configured with both `spec.config.dedicated_control_plane` and  `spec.system_components_placement`, which means that control plane node are dedicated(no workloads will be placed) and Rafay managed add-ons are placed on nodes with matching `labels` and `taints`

```terraform
resource "rafay_mks_cluster" "mks-ha-cluster-with-dedicated-cp" {
  api_version = "infra.k8smgmt.io/v3"
  kind        = "Cluster"
  metadata = {
    name    = "mks-ha-cluster-with-dedicated-cp"
    project = "terraform"
  }
  spec = {
    blueprint = {
      name = "minimal"
    }
    cloud_credentials = "mks-ssh-credentials"
    config = {
      auto_approve_nodes      = true
      high_availability       = true
      dedicated_control_plane = true
      kubernetes_version      = "v1.28.9"
      installer_ttl           = 365
      platform_version        = "v1.1.0"
      kubernetes_upgrade = {
        strategy = "sequential"
        params = {
          worker_concurrency = "50%"
        }
      }
      network = {
        cni = {
          name    = "Calico"
          version = "3.26.1"
        }
        pod_subnet     = "10.244.0.0/16"
        service_subnet = "10.96.0.0/12"
      }
      nodes = {
        "hostname1" = {
          arch             = "amd64"
          hostname         = "hostname1"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.233"
          roles            = ["ControlPlane"]
          labels = {
            "app"   = "infra"
            "infra" = "true"
          }
        },
        "hostname2" = {
          arch             = "amd64"
          hostname         = "hostname2"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.234"
          roles            = ["ControlPlane"]
        },
        "hostname3" = {
          arch             = "amd64"
          hostname         = "hostname3"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.25.235"
          roles            = ["ControlPlane"]
        },
        "hostname4" = {
          arch             = "amd64"
          hostname         = "hostname4"
          operating_system = "Ubuntu22.04"
          private_ip       = "10.12.114.236"
          roles            = ["Worker"]
          labels = {
            "app"   = "infra"
            "infra" = "true"
          }
          taints = [
            {
              effect = "NoSchedule"
              key    = "infra"
              value  = "true"
            },
            {
              effect = "NoSchedule"
              key    = "app"
              value  = "infra"
            },
          ]
        }
      }
    }
    system_components_placement = {
      node_selector = {
        "app"   = "infra"
        "infra" = "true"
      }
      tolerations = [
        {
          effect   = "NoSchedule"
          key      = "infra"
          operator = "Equal"
          value    = "true"
        },
        {
          effect   = "NoSchedule"
          key      = "app"
          operator = "Equal"
          value    = "infra"
        },
        {
          effect   = "NoSchedule"
          key      = "app"
          operator = "Equal"
          value    = "platform"
        },
      ]
    }
  }
}
```


### To upgrade the cluster

You can change the current Kubernetes version under `spec.config.kubernetes_version` to target supported version by Rafay and also customise the upgrade behaviour with `spec.config.kubernetes_upgrade`


<!-- schema generated by tfplugindocs -->
## Schema

**Required**

- `metadata` (Attributes) Contains data that helps uniquely identify the cluster (see [below for nested schema](#nestedatt--metadata))
- `spec` (Attributes) This is the desired state for the cluster and defines it's characteristics (see [below for nested schema](#nestedatt--spec))

**Optional**

- `api_version` (String) Api version for the cluster. Defaults to `infra.k8smgmt.io/v3`
- `kind` (String) Kind. Defaults to `Cluster`


<a id="nestedatt--metadata"></a>
### Nested Schema for `metadata`

**Required**

- `name` (String) The name of the cluster.
- `project` (String) The name of the Rafay project cluster will be created in

**Optional**
- `description` (String) Description for the cluster
- `labels` (Map of String) Key-value pairs containing metadata and are used to identify cluster
- `annotations` (Map of String) Annotations are extra non-identifying metadata associated with Cluster


<a id="nestedatt--spec"></a>
### Nested Schema for `spec`

**Required**

- `blueprint` (Attributes) The blueprint to be used for this cluster.  (see [below for nested schema](#nestedatt--spec--blueprint))
- `config` (Attributes) "Contains cluster configuration such as Kubernetes version, network configuration, etc. (see [below for nested schema](#nestedatt--spec--config))


**Optional**

- `cloud_credentials` (String) The SSH credentials to be used run  bootstrap cmds for node discovery. It's required if [spec.config.cluster_ssh](#nestedatt--spec--config--cluster_ssh) is not provided.
- `proxy` (Attributes) The proxy to be used for this cluster. (see [below for nested schema](#nestedatt--spec--proxy))
- `sharing` (Attributes) Sharing spec to be used for sharing the cluster with projects  (see [below for nested schema](#nestedatt--spec--sharing))
- `system_components_placement` (Attributes) Option to place Rafay Managed Add-ons and core components on Nodes with matching taints and labels. (see [below for nested schema](#nestedatt--spec--system_components_placement))
- `type` (String) The cluster type. Defaults to `mks`


<a id="nestedatt--spec--blueprint"></a>
### Nested Schema for `spec.blueprint`

**Required**

- `name` (String) Name of the blueprint 

**Optional**

- `version` (String) Version of the blueprint


<a id="nestedatt--spec--config"></a>
### Nested Schema for `spec.config`

**Required**

- `kubernetes_version` (String) Configure Kubernetes version of the Control Plane.
- `network` (Attributes) Contains network specification for the cluster (see [below for nested schema](#nestedatt--spec--config--network))
- `nodes` (Attributes Map) Contains configuration for each node in the cluster (see [below for nested schema](#nestedatt--spec--config--nodes))

**Optional**

- `installer_ttl` (Integer) By default, this setting allows ttl configuration for installer config. If not provided by default will set ttl to 365 days.
- `kubelet_extra_args` (Map of String) Cluster kubelet extra args.
- `auto_approve_nodes` (Boolean) By default, this setting allows incoming nodes to be automatically approved. It is recommended to set this option to true to avoid the need for manual approval for each node.
- `cluster_ssh` (Attributes). The default SSH Local configuration to run bootstrap commands for node discovery. It's required if `spec.cloud_credentials` are not provided(see [below for nested schema](#nestedatt--spec--config--cluster_ssh))
- `dedicated_control_plane` (Boolean) Select this option for preventing scheduling of user workloads on Control Plane nodes
- `high_availability` (Boolean) Select this option for highly available control plane. Minimum three control plane nodes are required
- `kubernetes_upgrade` (Attributes) Strategize the Kubernetes upgrade behaviour among the worker nodes (see [below for nested schema](#nestedatt--spec--config--kubernetes_upgrade))
- `location` (String) The data center location where the cluster nodes will be launched
- `platform_version` (String) Platform version that allows upgrading the cluster's internal components such as Cluster utils, Orchestration proxy, Orchestration agent, CRI, and etcd.
- `kubelet_configuration_overrides` (String) Define advanced kubelet settings using YAML. These configurations are validated automatically and form the base kubelet behavior. Use this for structured, persistent settings.

<a id="nestedatt--spec--config--network"></a>
### Nested Schema for `spec.config.network`

**Required**

- `cni` (Attributes) CNI Specification for the cluster (see [below for nested schema](#nestedatt--spec--config--network--cni))
- `pod_subnet` (String) Pods will be assigned IPs within this CIDR. For example: 10.244.0.0/16.
- `service_subnet` (String) Kuberenetes Services will be assigned IPs within this CIDR. For example: 10.244.0.0/16.

**Optional**

- `ipv6` (Attributes) Enable for Dual Stack support (see [below for nested schema](#nestedatt--spec--config--network--ipv6))

<a id="nestedatt--spec--config--network--cni"></a>
### Nested Schema for `spec.config.network.cni`

**Required**

- `name` (String) The CNI plugin to be used in the cluster. Supported plugins are `Calico` or `Cilium`
- `version` (String) Version of the CNI Plugin


<a id="nestedatt--spec--config--network--ipv6"></a>
### Nested Schema for `spec.config.network.ipv6`

**Optional**

- `pod_subnet` (String) Pods will be assigned IPs within this IPV6 CIDR. For example: 2001:db8:42:0::/56
- `service_subnet` (String) Kuberenetes Services will be assigned IPs within this CIDR. For ex: 2001:db8:42:1::/112



<a id="nestedatt--spec--config--nodes"></a>
### Nested Schema for `spec.config.nodes`

**Required**

- `arch` (String) System Architecture of the node
- `hostname` (String) Hostname of the node
- `operating_system` (String) Operating System of the node
- `private_ip` (String) Private ip address of the node
- `roles` (Set of String) Valid roles are: 'ControlPlane', 'Worker', 'Storage'

**Optional**

- `interface` (String) Interface to be used on the node
- `labels` (Map of String) Use Kubernetes labels to control how workloads are scheduled to your nodes.
- `kubelet_extra_args` (Map of String) Node kubelet extra args.
- `ssh` (Attributes) Override SSH Config at the node level. This is usefull when nodes within cluster come up with different SSH configuration.(see [below for nested schema](#nestedatt--spec--config--nodes--ssh))
- `taints` (Attributes Set) A node taint lets you mark a node so that the scheduler avoids or prevents using it for certain Pods. Node taints can be used with tolerations to ensure that Pods aren't scheduled onto inappropriate nodes  (see [below for nested schema](#nestedatt--spec--config--nodes--taints))
- `kubelet_configuration_overrides` (String) Define advanced kubelet settings using YAML. These configurations are validated automatically and form the base kubelet behavior. Use this for structured, persistent settings.

<a id="nestedatt--spec--config--nodes--ssh"></a>
### Nested Schema for `spec.config.nodes.ssh`

**Optional**

- `ip_address` (String) IP address to ssh into node
- `passphrase` (String) SSH Passphrase
- `port` (String) SSH Port
- `private_key_path` (String) Specify Path to SSH private key
- `username` (String) SSH Username


<a id="nestedatt--spec--config--nodes--taints"></a>
### Nested Schema for `spec.config.nodes.taints`

**Required**

- `key` (String) The taint key to be applied to a node.
- `effect` (String) The effect of the taint on pods that do not tolerate the taint. Valid effects are NoSchedule, PreferNoSchedule and NoExecute.

**Optional**

- `value` (String) The taint value corresponding to the taint key




<a id="nestedatt--spec--config--cluster_ssh"></a>
### Nested Schema for `spec.config.cluster_ssh`

**Optional**

- `passphrase` (String) Provide ssh passphrase
- `port` (String) Provide ssh port
- `private_key_path` (String) Provide local path to the private key
- `username` (String) Provide the ssh username


<a id="nestedatt--spec--config--kubernetes_upgrade"></a>
### Nested Schema for `spec.config.kubernetes_upgrade`

**Required**

- `params` (Attributes) (see [below for nested schema](#nestedatt--spec--config--kubernetes_upgrade--params))
- `strategy` (String) Kubernetes upgrade strategy for worker nodes and Valid options are: concurrent or sequential

<a id="nestedatt--spec--config--kubernetes_upgrade--params"></a>
### Nested Schema for `spec.config.kubernetes_upgrade.params`

**Required**

- `worker_concurrency` (String) It can be number of worker nodes or percentage of worker nodes to be upgraded at the same time




<a id="nestedatt--spec--proxy"></a>
### Nested Schema for `spec.proxy`

**Required**

- `enabled` (Boolean) Enable this option if your infrastructure is running behind a proxy
- `http_proxy` (String) Configure proxy information with protocol, host and port information.

**Optional**
- `allow_insecure_bootstrap` (Boolean) Select this option if proxy is terminating/inspecting TLS traffic
- `bootstrap_ca` (String) Root CA certificate of the proxy
- `https_proxy` (String) Configure proxy information with protocol, host and port information.
- `no_proxy` (String) Comma seperated list of hosts that need connectivity without proxy
- `proxy_auth` (String)


<a id="nestedatt--spec--sharing"></a>
### Nested Schema for `spec.sharing`

**Required**

- `enabled` (Boolean) Enable sharing for this resource.
- `projects` (Block List) The list of projects this resource is shared with. (see [below for nested schema](#nestedblock--spec--sharing--projects))

<a id="nestedblock--spec--sharing--projects"></a>
### Nested Schema for `spec.sharing.projects`

**Required**

- `name` (String) The name of the project to share the resource.



<a id="nestedatt--spec--system_components_placement"></a>
### Nested Schema for `spec.system_components_placement`

**Optional**
- `node_selector` (Map of String) Node selctors for pods that matches with node labels.
- `tolerations` (Attributes Set) Corresponding tolerations to match with Node taints (see [below for nested schema](#nestedatt--spec--system_components_placement--tolerations))
- `daemon_set_override` (Attributes) Enabling this allows to add additional tolerations for the Rafay daemon sets to match the taints available in the nodes. (see [below for nested schema](#nestedatt--spec--system_components_placement--daemon_set_override))




<a id="nestedatt--spec--system_components_placement--tolerations"></a>
### Nested Schema for `spec.system_components_placement.tolerations`

- `key` (String) The taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be `Exists`; this combination means to match all values and all keys.
- `operator` (String) Operator represents a key's relationship to the value. Valid operators are `Exists` and `Equal` . Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category.
- `value` (String) Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.
- `effect` (String) Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
- `toleration_seconds` (Number) TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute) tolerates the taint


<a id="nestedatt--spec--system_components_placement--daemon_set_override"></a>
### Nested Schema for `spec.system_components_placement.daemon_set_override`

**Optional**

- `daemon_set_tolerations` (Attributes Set) Tolerations for Rafay daemon sets (see [below for nested schema](#nestedatt--spec--system_components_placement--daemon_set_override--daemon_set_tolerations))
- `node_selection_enabled` (Boolean) Enable to  place Rafay daemon sets on nodes with matching labels only.

<a id="nestedatt--spec--system_components_placement--daemon_set_override--daemon_set_tolerations"></a>
### Nested Schema for `spec.system_components_placement.daemon_set_override.daemon_set_tolerations`

**Optional**

- `key` (String) The taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be `Exists`; this combination means to match all values and all keys.
- `operator` (String) Operator represents a key's relationship to the value. Valid operators are `Exists` and `Equal` . Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category.
- `value` (String) Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.
- `effect` (String) Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
- `toleration_seconds` (Number) TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute) tolerates the taint
