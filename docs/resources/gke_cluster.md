---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "rafay_gke_cluster Resource - terraform-provider-rafay"
subcategory: ""
description: |-
  
---

# rafay_gke_cluster (Resource)

## Example Usage

### Basic Public Cluster

```terraform
resource "rafay_gke_cluster" "gke-public-example" {
  metadata {
    name    = var.cluster_name
    project = var.rafay_project_name
  }
  spec {
    type = "gke"
    blueprint {
      name    = "minimal"
      version = "latest"
    }
    cloud_credentials = var.rafay_cloud_credential_name
    config {
      gcp_project           = var.gcp_project
      control_plane_version = "1.29"
      location {
        type = "zonal"
        config {
          zone = "us-central1-c"
        }
      }
      network {
        name                     = "default"
        subnet_name              = "default"
        enable_vpc_nativetraffic = "true"
        max_pods_per_node        = 110
        access {
          type = "public"
        }
      }
      features {
        enable_compute_engine_persistent_disk_csi_driver = "true"
        enable_cloud_logging                             = "true"
        cloud_logging_components                         = ["SYSTEM_COMPONENTS", "WORKLOADS"]
        enable_cloud_monitoring                          = "true"
        cloud_monitoring_components                      = ["SYSTEM_COMPONENTS"]
      }
      node_pools {
        name         = "default-nodepool"
        node_version = "1.29"
        size         = 3
        machine_config {
          machine_type   = "e2-standard-4"
          image_type     = "COS_CONTAINERD"
          boot_disk_type = "pd-standard"
          boot_disk_size = 100
        }
      }
    }
  }
}
```

### Private Cluster and firewall rules

```terraform
resource "rafay_gke_cluster" "gke-private-example" {
  metadata {
    name    = var.cluster_name
    project = var.rafay_project_name
  }
  spec {
    type = "gke"
    blueprint {
      name    = "minimal"
      version = "latest"
    }
    cloud_credentials = var.rafay_cloud_credential_name
    config {
      gcp_project           = var.gcp_project
      control_plane_version = "1.29"
      location {
        type = "zonal"
        config {
          zone = "us-central1-c"
        }
      }
      network {
        name                     = "default"
        subnet_name              = "default"
        enable_vpc_nativetraffic = "true"
        max_pods_per_node        = 110
        access {
          type = "private"
          config {
            control_plane_ip_range                  = "172.16.3.0/28"
            enable_access_control_plane_external_ip = "true"
            enable_access_control_plane_global      = "true"
            disable_snat                            = "true"
            firewall_rules {
              name        = "allow-internal"
              description = "allow traffic"
              action      = "allow"
              direction   = "INGRESS"
              priority    = 1000
              source_ranges = [
                "10.128.0.0/9"
              ]
              rules {
                ports    = ["443", "80"]
                protocol = "tcp"
              }
            }
          }
        }
      }
      features {
        enable_compute_engine_persistent_disk_csi_driver = "true"
        enable_cloud_logging                             = "true"
        cloud_logging_components                         = ["SYSTEM_COMPONENTS", "WORKLOADS"]
        enable_cloud_monitoring                          = "true"
        cloud_monitoring_components                      = ["SYSTEM_COMPONENTS"]
      }
      node_pools {
        name         = "default-nodepool"
        node_version = "1.29"
        size         = 3
        machine_config {
          machine_type   = "e2-standard-4"
          image_type     = "COS_CONTAINERD"
          boot_disk_type = "pd-standard"
          boot_disk_size = 100
        }
      }
    }
  }
}
```

### Shared VPC Cluster

```terraform
resource "rafay_gke_cluster" "gke-shared-vpc-example" {
  metadata {
    name    = var.cluster_name
    project = var.rafay_project_name
  }
  spec {
    type = "gke"
    blueprint {
      name    = "minimal"
      version = "latest"
    }
    cloud_credentials = var.rafay_cloud_credential_name
    config {
      gcp_project           = var.gcp_project
      control_plane_version = "1.29"
      location {
        type = "zonal"
        config {
          zone = "us-central1-c"
        }
      }
      network {
        name                         = "projects/${var.gcp_project}/global/networks/shared-net"
        subnet_name                  = "projects/${var.gcp_project}/regions/us-central1/subnetworks/shared-subnet"
        pod_secondary_range_name     = "pods-range"
        service_secondary_range_name = "services-range"
        enable_vpc_nativetraffic     = "true"
        max_pods_per_node            = 110
        access {
          type = "public"
        }
      }
      features {
        enable_compute_engine_persistent_disk_csi_driver = "true"
        enable_cloud_logging                             = "true"
        cloud_logging_components                         = ["SYSTEM_COMPONENTS", "WORKLOADS"]
        enable_cloud_monitoring                          = "true"
        cloud_monitoring_components                      = ["SYSTEM_COMPONENTS"]
      }
      node_pools {
        name         = "default-nodepool"
        node_version = "1.29"
        size         = 3
        machine_config {
          machine_type   = "e2-standard-4"
          image_type     = "COS_CONTAINERD"
          boot_disk_type = "pd-standard"
          boot_disk_size = 100
        }
      }
    }
  }
}
```

### Cluster having node pool using reservation affinity

```terraform
resource "rafay_gke_cluster" "gke-reservation-affinity-example" {
  metadata {
    name    = var.cluster_name
    project = var.rafay_project_name
  }
  spec {
    type = "gke"
    blueprint {
      name    = "minimal"
      version = "latest"
    }
    cloud_credentials = var.rafay_cloud_credential_name
    config {
      gcp_project           = var.gcp_project
      control_plane_version = "1.29"
      location {
        type = "zonal"
        config {
          zone = "us-central1-c"
        }
      }
      network {
        name                     = "default"
        subnet_name              = "default"
        enable_vpc_nativetraffic = "true"
        max_pods_per_node        = 110
        access {
          type = "public"
        }
      }
      features {
        enable_compute_engine_persistent_disk_csi_driver = "true"
        enable_cloud_logging                             = "true"
        cloud_logging_components                         = ["SYSTEM_COMPONENTS", "WORKLOADS"]
        enable_cloud_monitoring                          = "true"
        cloud_monitoring_components                      = ["SYSTEM_COMPONENTS"]
      }
      node_pools {
        name         = "default-nodepool"
        node_version = "1.29"
        size         = 3
        machine_config {
          machine_type   = "e2-standard-4"
          image_type     = "COS_CONTAINERD"
          boot_disk_type = "pd-standard"
          boot_disk_size = 100
          reservation_affinity {
            consume_reservation_type = "specific"
            reservation_name         = "my-reservation"
          }
        }
      }
    }
  }
}
```

### Cluster having GPU nodes

```terraform
resource "rafay_gke_cluster" "gke-gpu-example" {
  metadata {
    name    = var.cluster_name
    project = var.rafay_project_name
  }
  spec {
    type = "gke"
    blueprint {
      name    = "minimal"
      version = "latest"
    }
    cloud_credentials = var.rafay_cloud_credential_name
    config {
      gcp_project           = var.gcp_project
      control_plane_version = "1.29"
      location {
        type = "zonal"
        config {
          zone = "us-central1-c"
        }
      }
      network {
        name                     = "default"
        subnet_name              = "default"
        enable_vpc_nativetraffic = "true"
        max_pods_per_node        = 110
        access {
          type = "public"
        }
      }
      features {
        enable_compute_engine_persistent_disk_csi_driver = "true"
        enable_cloud_logging                             = "true"
        cloud_logging_components                         = ["SYSTEM_COMPONENTS", "WORKLOADS"]
        enable_cloud_monitoring                          = "true"
        cloud_monitoring_components                      = ["SYSTEM_COMPONENTS"]
      }
      node_pools {
        name         = "default-nodepool"
        node_version = "1.29"
        size         = 3
        machine_config {
          machine_type   = "n1-standard-4"
          image_type     = "COS_CONTAINERD"
          boot_disk_type = "pd-standard"
          boot_disk_size = 100
          accelerators {
            type  = "nvidia-tesla-t4"
            count = 1
            gpu_driver_installation {
              config {
                version = "LATEST"
              }
              type = "google-managed"
            }
          }
        }
      }
    }
  }
}
```


<!-- schema generated by tfplugindocs -->
## Argument Reference

### Required

- `metadata` - (Block List, Max: 1) Contains data that helps uniquely identify the resource. (See [below for nested schema](#nestedblock--metadata))
- `spec` - (Block List, Max: 1) Defines the characteristics for the resource. This is the desired state for the resource. (See [below for nested schema](#nestedblock--spec))

### Optional

- `timeouts` - (Block) Sets the duration of time the create, delete, and update functions are allowed to run. If the function takes longer than this, it is assumed the function has failed. The default is 10 minutes. (See [below for nested schema](#nestedblock--timeouts))


### Read-Only

- `id` (String) The ID of this resource.

<a id="nestedblock--metadata"></a>
### Nested Schema for `metadata`


***Required***

- `name` - (String) The name of the Cluster. This must be unique in your organization.
- `project` - (String) The name of the Rafay project the cluster will be created in.

***Optional***

- `annotations` (Map of String) annotations of the resource
- `description` (String) description of the resource
- `labels` (Map of String) labels of the resource

<a id="nestedblock--spec"></a>
### Nested Schema for `spec`


***Required***

- `cloud_credentials` - (String) The name of the cloud credentials used to create and manage the cluster.
- `config` - (Block List, Min: 1) The GKE specific cluster configuration. (See [below for nested schema](#nestedblock--spec--config))
- `type` - (String) Cluster type. The supported value is `gke`.
- `blueprint` (Block List, Max: 1) The blueprint to be used for this cluster. (see [below for nested schema](#nestedblock--spec--blueprint))

***Optional***

- `proxy` (Block List, Max: 1) The proxy to be used for this cluster. (see [below for nested schema](#nestedblock--spec--proxy))
- `sharing` (Block List, Max: 1) Sharing spec to be used for sharing the cluster with projects (see [below for nested schema](#nestedblock--spec--sharing))

<a id="nestedblock--spec--blueprint"></a>
### Nested Schema for `spec.blueprint`


- `name` (String) The name of the blueprint to be associated with the cluster. A blueprint defines the configuration and policy. Use blueprint to help standardize cluster configurations. 
- `version` (String) The blueprint version to be associated with the cluster. 

<a id="nestedblock--spec--proxy"></a>
### Nested Schema for `spec.proxy`

***Required***

- `enabled` (Boolean) Enable this option if your infrastructure is running behind a proxy
- `http_proxy` (String) Configure proxy information with protocol, host and port information.

***Optional***

- `allow_insecure_bootstrap` (Boolean) Select this option if proxy is terminating/inspecting TLS traffic
- `bootstrap_ca` (String) Root CA certificate of the proxy
- `https_proxy` (String) Configure proxy information with protocol, host and port information.
- `no_proxy` (String) Comma seperated list of hosts that need connectivity without proxy
- `proxy_auth` (String)

<a id="nestedblock--spec--sharing"></a>
### Nested Schema for `spec.sharing`

***Required***

- `enabled` (Boolean) Enable sharing for this resource.
- `projects` (Block List) The list of projects this resource is shared with. (see [below for nested schema](#nestedblock--spec--sharing--projects))

<a id="nestedblock--spec--sharing--projects"></a>
### Nested Schema for `spec.sharing.projects`

***Required***

- `name` (String) The name of the project to share the resource.


<a id="nestedblock--spec--config"></a>
### Nested Schema for `spec.config`

***Required***

- `gcp_project` (String) GCP Project name.
- `control_plane_version` (String) Kubernetes version of ControlPlane
- `location` (Block List, Max: 1) Cluster location configuration. (see [below for nested schema](#nestedblock--spec--config--location))
- `network` (Block List, Max: 1) Cluster Network configuration. (see [below for nested schema](#nestedblock--spec--config--network))
- `node_pools` (Block List) Cluster node pool configuration. (see [below for nested schema](#nestedblock--spec--config--node_pools))

Optional:

- `features` (Block List, Max: 1) Cluster additional features configuration. (see [below for nested schema](#nestedblock--spec--config--features))
- `pre_bootstrap_commands` (List of String) Pre-bootstrap commands is a list of (one of more) commands that the user wants run on their target cluster. These commands will be run every time a node comes up, both during cluster creation and cluster/nodepool scale. Example: Node restart and node creation. Refer [preBootstrapCommands Guidelines on Rafay doc](https://docs.rafay.co/clusters/gke/preboot_commands/#prebootstrapcommands-guidelines) for usage. Special case: `${ROOT_DIR}` is used to refer root directory of nodes. However `${ROOT_DIR}` conflicts with Terraform's template syntax so you'd need to escape it by writting `$${ROOT_DIR}` instead.
- `security` (Block List, Max: 1) Cluster security configuration. (see [below for nested schema](#nestedblock--spec--config--security))


<a id="nestedblock--spec--config--location"></a>
### Nested Schema for `spec.config.location`

***Required***

- `type` (String) Cluster location can be either `zonal` or `regional`
- `config` (Block List, Max: 1) (see [below for nested schema](#nestedblock--spec--config--location--config))

***Optional***

- `default_node_locations` (Block List, Max: 1) 
For Zonal cluster increase availability by selecting more than one zone. 
For Regional cluster, by default Kubernetes Engine runs nodes of a regional cluster across three zones within a region. Select this option if you want to manually specify the zones in which this cluster's nodes run. All zones must be within the same region. (see [below for nested schema](#nestedblock--spec--config--location--default_node_locations))

<a id="nestedblock--spec--config--location--config"></a>
### Nested Schema for `spec.config.location.config`

For ZonalCluster only zone information is sufficient. For Regional Cluster, both region and zone should be provided.

- `region` (String) Regional location in which the cluster's control plane and nodes are located.
- `zone` (String) Zone in the region where bootstrap VM is created for cluster provisioning. 


<a id="nestedblock--spec--config--location--default_node_locations"></a>
### Nested Schema for `spec.config.location.default_node_locations`

- `enabled` (Boolean) Enable providing default node locations
- `zones` (List of String) List of zones. Increase availability by providing more than one zone. The same number of nodes will be deployed to each zone in the list.


<a id="nestedblock--spec--config--network"></a>
### Nested Schema for `spec.config.network`

***Required***

- `name` (String) Name of the network that the cluster is in. It determines which other Compute Engine resource it is able to communicate with
- `subnet_name` (String) Subnetwork to which the Kubernetes cluster will belong. When VPC native is enabled, the subnetwork must contain at least two secondary ranges which are not used by other Kubernetes clusters. Subnet is permanent.
- `access` (Block List, Max: 1) NetworkAccess config for describing access configurations for the cluster's workload (see [below for nested schema](#nestedblock--spec--config--network--access))

***Optional***

- `control_plane_authorized_network` (Block List, Max: 1) Add control plane authorized networks to block untrusted non-GCP source IPs from accessing the Kubernetes control plane through HTTPS (see [below for nested schema](#nestedblock--spec--config--network--control_plane_authorized_network))
- `enable_vpc_nativetraffic` (Boolean) This feature uses alias IP and provides a more secure integration with Google Cloud Platform services
- `max_pods_per_node` (Number) This value is used to optimize the partitioning of cluster's IP address range to sub-ranges at node level
- `pod_address_range` (String) All pods in the cluster are assigned an IP address from this range. Enter a range (in CIDR notation) within a network range, a mask, or leave this field blank to use a default range.
- `service_address_range` (String) Cluster services will be assigned an IP address from this IP address range. Enter a range (in CIDR notation) within a network range, a mask, or leave this field blank to use a default range.
- `pod_secondary_range_name` (String) Cluster pods are assigned an IP from the selected node subnet's secondary CIDR address range.
- `service_secondary_range_name` (String) Cluster services are assigned an IP from the selected node subnetes secondary CIDR address range.
- `data_plane_v_2` (String) Dataplane V2 is optimized for Kubernetes networking which is implemented using eBPF. Supported value is `ADVANCED_DATAPATH`. 
- `enable_data_plane_v_2_metrics` (Boolean) Dataplane V2 metrics brings better insight into the traffic between your Kubernetes workloads. Understand how your services communicate, identify issues with the network health, verify Kubernetes policies and more.
- `enable_data_plane_v_2_observability` (Boolean) Dataplane V2 observability provides Managed Hubble CLI solution that lets you observe network flows between your Kubernetes workloads in real time. Check out here for more https://cloud.google.com/kubernetes-engine/docs/concepts/about-dpv2-observability
- `network_policy_config` (Boolean) Configuration for NetworkPolicy. This only tracks whether the addon is enabled or not on the Master, it does not track whether network policy is enabled for the nodes.
- `network_policy` (String) The Kubernetes Network Policy API allows the cluster administrator to specify what pods are allowed to communicate with each other. Cannot use with `data_plane_v_2` config. Supported value is `CALICO`.

<a id="nestedblock--spec--config--network--access"></a>
### Nested Schema for `spec.config.network.access`

***Required***

-  `type` (String) Choose the type of network you want to allow to access your cluster's workloads. `private` or `public`
- `config` (Block List, Max: 1) For `public` cluster config is null. But for `private` cluster, some settings are required. (see [below for nested schema](#nestedblock--spec--config--network--access--config))


<a id="nestedblock--spec--config--network--access--config"></a>
### Nested Schema for `spec.config.network.access.config`

***Required For Private Cluster***

- `control_plane_ip_range` (String) This is required for private cluster type. Control plane IP range is for the control plane VPC. The control plane range must not overlap with any subnet in your cluster's VPC. The control plane and your cluster use VPC peering to communicate privately
- `enable_access_control_plane_external_ip` (Boolean) Disabling this option locks down external access to the cluster control plane. There is still an external IP address used by Google for cluster management purposes, but the IP address is not accessible to anyone. If this is disabled, `control_plane_authorized_network` must be configured for Private Cluster.

***Optional***

- `disable_snat` (Boolean) To use Privately Used Public IPs (PUPI) ranges, the default source NAT used for IP masquerading needs to be disabled
- `enable_access_control_plane_global` (Boolean) With control plane global access, you can access the control plane's private endpoint from any GCP region or on-premises environment no matter what the private cluster's region is.
- `firewall_rules` (Block List) Use FirewallRule config to specify additional firewall rules. Only Private clusters are supported.By default, tcp:9443 and tcp:22281 are opened for private cluster. (see [below for nested schema](#nestedblock--spec--config--network--access--config--firewall_rules))

<a id="nestedblock--spec--config--network--access--config--firewall_rules"></a>
### Nested Schema for `spec.config.network.access.config.firewall_rules`

Optional:

- `action` (String)
- `description` (String)
- `destination_ranges` (List of String)
- `direction` (String)
- `name` (String)
- `network` (String)
- `priority` (Number)
- `rules` (Block List) (see [below for nested schema](#nestedblock--spec--config--network--access--config--firewall_rules--rules))
- `source_ranges` (List of String)
- `target_tags` (List of String)

<a id="nestedblock--spec--config--network--access--config--firewall_rules--rules"></a>
### Nested Schema for `spec.config.network.access.config.firewall_rules.rules`

Optional:

- `ports` (List of String)
- `protocol` (String)

<a id="nestedblock--spec--config--network--control_plane_authorized_network"></a>
### Nested Schema for `spec.config.network.control_plane_authorized_network`

- `enabled` (Boolean) Enable Control Plane Authorized Network. Configure the Networks now or later.

***Optional***

- `authorized_network` (Block List) Add control plane authorized networks to block untrusted non-GCP source IPs from accessing the Kubernetes control plane through HTTPS (see [below for nested schema](#nestedblock--spec--config--network--subnet_name--authorized_network))

<a id="nestedblock--spec--config--network--subnet_name--authorized_network"></a>
### Nested Schema for `spec.config.network.subnet_name.authorized_network`

***Required***

- `cidr` (String) CIDR Example: 198.51.100.0/24

***Optional***

- `name` (String) Name of the Authorized Network Example: Corporate Office


<a id="nestedblock--spec--config--features"></a>
### Nested Schema for `spec.config.features`

- `enable_compute_engine_persistent_disk_csi_driver` (Boolean) Enable to automatically deploy and manage the Compute Engine Persistent Disk CSI Driver. This feature is an alternative to using the gcePersistentDisk in-tree volume plugin. From controlplane k8s >1.25, this setting should be enabled and is mandatory.

***Optional***

- `cloud_logging_components` (List of String) List of components for cloud logging. Values: "SYSTEM_COMPONENTS", "WORKLOADS".
- `cloud_monitoring_components` (List of String) List of components for cloud monitoring. Values: "SYSTEM_COMPONENTS"
- `enable_application_manager_beta` (Boolean) Application Manager is a GKE controller for managing the lifecycle of applications. It enables application delivery and updates following Kubernetes and GitOps best practices
- `enable_backup_for_gke` (Boolean) Backup for GKE allows you to back up and restore workloads. There is no cost for enabling this feature, but you are charged for backups based on the size of the data and the number of pods you protect
- `enable_cloud_logging` (Boolean) Logging collects logs emitted by your applications and by GKE infrastructure
- `enable_cloud_monitoring` (Boolean) Monitoring collects metrics emitted by your applications and by GKE infrastructure
- `enable_filestore_csi_driver` (Boolean) Enable to automatically deploy and manage the Filestore CSI Driver
- `enable_image_streaming` (Boolean) Image streaming allows your workloads to initialize without waiting for the entire image to download
- `enable_managed_service_prometheus` (Boolean) This option deploys managed collectors for Prometheus metrics within this cluster. These collectors must be configured using PodMonitoring resources. To enable Managed Service for Prometheus here, you'll need. Cluster version of 1.21.4-gke.300 or greater


<a id="nestedblock--spec--config--node_pools"></a>
### Nested Schema for `spec.config.node_pools`

***Required***

- `name` (String) Node pool names must start with a lowercase letter followed by up to 39 lowercase letters, numbers, or hyphens. They can't end with a hyphen. You cannot change the node pool's name once it's created
- `node_version` (String) Specify Node k8s version
- `size` (Number) Pod address range limits the maximum size of the cluster
- `machine_config` (Block List, Max: 1) Choose the machine configuration that will best fit the resource needs of your cluster (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config))


***Optional***

- `networking` (Block List, Max: 1) Node networking settings will be used when new nodes are created using this node pool (see [below for nested schema](#nestedblock--spec--config--node_pools--networking))
- `node_locations` (Block List, Max: 1) Additional node zones must be from the same region as the original zone. Kubernetes Engine allocates the same resource footprint for each zone. The Node pool setting overrides the defaults set in Cluster basics (see [below for nested schema](#nestedblock--spec--config--node_pools--node_locations))
- `auto_scaling` (Block List, Max: 1) Cluster autoscaler automatically creates or deletes nodes based on workload needs (see [below for nested schema](#nestedblock--spec--config--node_pools--auto_scaling))
- `metadata` (Block List, Max: 1) Node metadata settings will be used when new nodes are created using this node pool (see [below for nested schema](#nestedblock--spec--config--node_pools--metadata))
- `security` (Block List, Max: 1) Node security settings will be used when new nodes are created using this node pool (see [below for nested schema](#nestedblock--spec--config--node_pools--security))
- `management` (Block List, Max: 1) Node management configuration (see [below for nested schema](#nestedblock--spec--config--node_pools--management))
- `upgrade_settings` (Block List, Max: 1) Node pool upgrade options (see [below for nested schema](#nestedblock--spec--config--node_pools--upgrade_settings))


<a id="nestedblock--spec--config--node_pools--machine_config"></a>
### Nested Schema for `spec.config.node_pools.machine_config`

***Optional***

- `boot_disk_size` (Number) Select Boot disk size. Boot disk size is permanent
- `boot_disk_type` (String) Select Boot disk type. Storage space is less expensive for a standard persistent disk. An SSD persistent disk is better for random IOPS or for streaming throughput with low latency
- `image_type` (String) Choose which operating system image you want to run on each node of this cluster
- `machine_type` (String) Choose the machine type that will best fit the resource needs of your cluster
- `reservation_affinity` (Block List, Max: 1) Zonal compute reservation to this node pool (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config--reservation_affinity))
- `accelerators` (Block List, Max: 1) Configure GPU config to this node pool (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config--accelerators))

<a id="nestedblock--spec--config--node_pools--machine_config--accelerators"></a>
### Nested Schema for `spec.config.node_pools.machine_config.accelerators`

***Required***

- `type` (String) Type of accelerator. Example: nvidia-tesla-k80
- `count` (Number) Number of accelerator. Example: 1
- `gpu_driver_installation` (Block List, Max: 1) GPU driver installation settings (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config--gpu_driver_installation))

***Optional***

- `gpu_partition_size` (String) GPU partition size. Example: 1g.5gb
- `accelerator_sharing` (Block List, Max: 1) Accelerator sharing settings (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config--accelerator_sharing))

<a id="nestedblock--spec--config--node_pools--machine_config--accelerator_sharing"></a>
### Nested Schema for `spec.config.node_pools.machine_config.accelerator_sharing`

***Required***

- `strategy` (String) Accelerator sharing strategy. Example: TIME_SHARING
- `max_shared_clients` (Number) Maximum number of shared clients. Example: 2

<a id="nestedblock--spec--config--node_pools--machine_config--gpu_driver_installation"></a>
### Nested Schema for `spec.config.node_pools.machine_config.gpu_driver_installation`

***Required***

- `type` (String) Type of GPU driver. Example: google-managed, user-managed

***Optional***

- `config` (Block List, Max: 1) GPU driver installation settings (see [below for nested schema](#nestedblock--spec--config--node_pools--machine_config--gpu_driver_installation--config))

<a id="nestedblock--spec--config--node_pools--machine_config--gpu_driver_installation--config"></a>
### Nested Schema for `spec.config.node_pools.machine_config.gpu_driver_installation.config`

***Required***

- `version` (String) GPU driver version. Example: DEFAULT/LATEST. Only supported for installation type `google-managed`.


<a id="nestedblock--spec--config--node_pools--machine_config--reservation_affinity"></a>
### Nested Schema for `spec.config.node_pools.machine_config.reservation_affinity`

***Optional***

- `consume_reservation_type` (String) Type of reservation consumption.
- `reservation_name` (String) The name of the Reservation to be consumed. Only mandatory when consumeReservationType is set to specific

<a id="nestedblock--spec--config--node_pools--networking"></a>
### Nested Schema for `spec.config.node_pools.networking`

***Optional***

- `max_pods_per_node` (Number) This value is used to optimize the partitioning of cluster's IP address range to sub-ranges at node level.This setting is permanent.
- `network_tags` (List of String) Tags represent firewall rules applied to each node.


<a id="nestedblock--spec--config--node_pools--node_locations"></a>
### Nested Schema for `spec.config.node_pools.node_locations`

***Optional***

- `enabled` (Boolean) Enable providing node locations
- `zones` (List of String) List of zones. Additional node zones must be from the same region as the original zone. Kubernetes Engine allocates the same resource footprint for each zone. The Node pool setting overrides the defaults set in Cluster basics


<a id="nestedblock--spec--config--node_pools--security"></a>
### Nested Schema for `spec.config.node_pools.security`

***Optional***

- `enable_integrity_monitoring` (Boolean) Integrity monitoring lets you monitor and verify the runtime boot integrity of your shielded nodes using Cloud Monitoring
- `enable_secure_boot` (Boolean) Secure boot helps protect your nodes against boot-level and kernel-level malware and rootkits


<a id="nestedblock--spec--config--node_pools--auto_scaling"></a>
### Nested Schema for `spec.config.node_pools.auto_scaling`

***Optional***

- `max_nodes` (Number) Maximum number of nodes (per zone)
- `min_nodes` (Number) Minimum number of nodes (per zone)

<a id="nestedblock--spec--config--node_pools--metadata"></a>
### Nested Schema for `spec.config.node_pools.metadata`

***Optional***

- `gce_instance_metadata` (Block List) Metadata to be stored in the instance (see [below for nested schema](#nestedblock--spec--config--node_pools--metadata--gce_instance_metadata))
- `kubernetes_labels` (Block List) Use Kubernetes labels to control how workloads are scheduled to your nodes. Labels are applied to all nodes in this node pool and cannot be changed once the cluster is created (see [below for nested schema](#nestedblock--spec--config--node_pools--metadata--kubernetes_labels))
- `node_taints` (Block List) A node taint lets you mark a node so that the scheduler avoids or prevents using it for certain Pods. Node taints can be used with tolerations to ensure that Pods aren't scheduled onto inappropriate nodes (see [below for nested schema](#nestedblock--spec--config--node_pools--metadata--node_taints))

<a id="nestedblock--spec--config--node_pools--metadata--gce_instance_metadata"></a>
### Nested Schema for `spec.config.node_pools.metadata.gce_instance_metadata`

***Optional***

- `key` (String) Key for this metadata
- `value` (String) Value for this metadata


<a id="nestedblock--spec--config--node_pools--metadata--kubernetes_labels"></a>
### Nested Schema for `spec.config.node_pools.metadata.kubernetes_labels`

***Optional***

- `key` (String) Key for this kubernetes label
- `value` (String) Value for this kubernetes lable


<a id="nestedblock--spec--config--node_pools--metadata--node_taints"></a>
### Nested Schema for `spec.config.node_pools.metadata.node_taints`

***Optional***

- `effect` (String) Available effects are `NoSchedue`, `PreferNoSchedule`, `NoExecute`
- `key` (String) Key for this Taint effect
- `value` (String) Value for this Taint effect


<a id="nestedblock--spec--config--security"></a>
### Nested Schema for `spec.config.security`

- `enable_google_groups_for_rbac` (Boolean) Google Groups for RBAC allows you to grant roles to all members of a Google Workspace group
- `enable_legacy_authorization` (Boolean) Enable legacy authorization to support in-cluster permissions for existing clusters or workflows. Prevents full RBAC support
- `enable_workload_identity` (Boolean) Workload Identity lets you connect securely to Google APIs from Kubernetes Engine workloads
- `issue_client_certificate` (Boolean) Clients use this base64-encoded public certificate to authenticate to the cluster endpoint. Certificates don’t rotate automatically and are difficult to revoke
- `security_group` (String) Provide the security groups here


<a id="nestedblock--spec--config--node_pools--management"></a>
### Nested Schema for `spec.config.node_pools.management`

- `auto_upgrade` (Boolean) Whether the nodes will be automatically upgraded


<a id="nestedblock--spec--config--node_pools--upgrade_settings"></a>
### Nested Schema for `spec.config.node_pools.upgrade_settings`

***Required***
- `strategy` (String) Update strategy of the node pool. Possible values are `SURGE` or `BLUE_GREEN`

- `blue_green_settings` (Block List, Max: 1) If `strategy` is `BLUE_GREEN`, configure this setting for upgrade strategy (see [below for nested schema](#nestedblock--spec--config--node_pools--upgrade_settings--blue_green_settings))
- `surge_settings` (Block List, Max: 1) If `strategy` is `SURGE`, configure this setting for upgrade strategy (see [below for nested schema](#nestedblock--spec--config--node_pools--upgrade_settings--surge_settings))

<a id="nestedblock--spec--config--node_pools--upgrade_settings--blue_green_settings"></a>
### Nested Schema for `spec.config.node_pools.upgrade_settings.blue_green_settings`

***Required***

- `batch_node_count` (Number) The absolute number of nodes to drain in a batch. If it is set to zero, this phase will be skipped.
- `batch_soak_duration` (String) Duration in seconds to wait after each batch finishes draining. This is an opportunity to verify your workload’s health for the batch upgraded.
- `node_pool_soak_duration` (String) Duration in seconds to wait when all batches are completely drained. This is an opportunity to verify your workload’s health.


<a id="nestedblock--spec--config--node_pools--upgrade_settings--surge_settings"></a>
### Nested Schema for `spec.config.node_pools.upgrade_settings.surge_settings`

***Required***

- `max_surge` (Number) The maximum number of nodes that can be created beyond the current size of the node pool during the upgrade process.
- `max_unavailable` (Number) The maximum number of nodes that can be simultaneously unavailable during the upgrade process.


<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

***Optional***

- `create` - (String) Sets the timeout duration for creating a resource. 
- `delete` - (String) Sets the timeout duration for deleting a resource. 
- `update` - (String) Sets the timeout duration for updating a resource.



# rafay_gke_cluster (data source)
## Example Usage

---

```terraform
data "rafay_gke_cluster" "cluster" {
  metadata {
	name    = "cluster-gke"
	project = "demo"
  }
}
  
output "gke_cluster" {
  description = "gke_cluster"
  value       = data.rafay_gke_cluster.cluster
}
```

---
